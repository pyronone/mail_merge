[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mail_merge",
    "section": "",
    "text": "In 2 years at TELUS Health / LifeWorks, I’ve probably done over a dozen mail merges of varying sizes. With a natural proclivity for process improvement and efficiency, I mainly used Python scripts to streamline the data cleaning and joining part of the process.\nFor the sake of improved organization, maintainability, and documentation, I have bundled commonly used functions from those scripts into a single module. I also experimented with using docxtpl for the actual merge as the standard, built-in MS Word function proved unreliable/inconsistent, especially for larger merges.\nThis package allows for effortless running and re-running of merges with manual steps kept to a minimum. This is particularly useful when changes to templates and/or data are required, and when there are different groups requiring different templates. More features will be added over time.",
    "crumbs": [
      "mail_merge"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "00_core",
    "section": "",
    "text": "00 init fxs\n\nsource\n\nyaml_helper\n\n yaml_helper (fpath:str='./config.yaml', mode:str='r', data:dict=None)\n\nHelper function to read, write, append to files in yaml format. Checks for duplicate keys if reading or appending.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfpath\nstr\n./config.yaml\nstr\n\n\nmode\nstr\nr\nstr - r / a / w\n\n\ndata\ndict\nNone\ndict - cannot be None if writing or appending\n\n\nReturns\ndict\n\ndict - data if reading, {‘r’: 0} if writing/appending\n\n\n\n\nsource\n\n\nfp\n\n fp (relative_fp:str, base_dir:str='.')\n\nFor referencing filepaths relative to pkg dir.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nrelative_fp\nstr\n\nstr - eg. “../dir/file.txt”\n\n\nbase_dir\nstr\n.\nstr\n\n\nReturns\nstr\n\nstr\n\n\n\n\n\n\n01 format numbers\n\nsource\n\nFormatOptions\n\n FormatOptions (format:bool=True, prefix:str='', suffix:str='',\n                comma:bool=True, comma_str:str=',',\n                pad_zeroes_at_end:bool=True,\n                return_int_if_possible:bool=False, decimal_str:str='.')\n\n\n\nCode\n# * DOC\nFormatOptions()\n\n\nFormatOptions(format=True, prefix='', suffix='', comma=True, comma_str=',', pad_zeroes_at_end=True, return_int_if_possible=False, decimal_str='.')\n\n\n\nsource\n\n\nfix_round_and_format\n\n fix_round_and_format (num:float, position:int,\n                       format_opts:__main__.FormatOptions)\n\ndeprecated - use round_and_format\n\n\n\n\nType\nDetails\n\n\n\n\nnum\nfloat\nfloat\n\n\nposition\nint\nint\n\n\nformat_opts\nFormatOptions\nFormatOptions\n\n\nReturns\ntyping.Union[str, float]\nUnion[str, float]\n\n\n\n\n\nCode\n# * TEST\nassert fix_round_and_format(123.445, 2, FormatOptions()) == \"123.45\"\nassert fix_round_and_format(9123.445, 2, FormatOptions(comma=True)) == \"9,123.45\"\nassert fix_round_and_format(123.445, 2, FormatOptions(comma=False)) == \"123.45\"\nassert (\n    fix_round_and_format(123.445, 4, FormatOptions(comma=False, pad_zeroes_at_end=True))\n    == \"123.4450\"\n)\nassert fix_round_and_format(-2342.475, 2, FormatOptions()) == \"-2,342.48\"\n\nassert (\n    fix_round_and_format(2342.999, 0, FormatOptions(return_int_if_possible=True))\n    == \"2,343\"\n)\nassert (\n    fix_round_and_format(2342.999, 0, FormatOptions(return_int_if_possible=False))\n    == \"2,343.0\"\n)\nassert (\n    fix_round_and_format(2342.999, 0, FormatOptions(return_int_if_possible=False))\n    == \"2,343.0\"\n)\nassert fix_round_and_format(2342.999, 0, FormatOptions(comma=False)) == \"2343.0\"\n\nassert (\n    fix_round_and_format(2342.999, 1, FormatOptions(return_int_if_possible=True))\n    == \"2,343\"\n)\nassert (\n    fix_round_and_format(\n        2342.999, 1, FormatOptions(return_int_if_possible=True, comma=False)\n    )\n    == \"2343\"\n)\nassert (\n    fix_round_and_format(2342.999, 1, FormatOptions(return_int_if_possible=False))\n    == \"2,343.0\"\n)\n\nassert fix_round_and_format(0.0, 0, FormatOptions()) == \"0.0\"\nassert fix_round_and_format(\"0\", 0, FormatOptions()) == \"0\"\nassert fix_round_and_format(\"str\", 0, FormatOptions()) == \"str\"\n\nassert pd.isnull(fix_round_and_format(np.nan, 2, FormatOptions()))\n\n\n\n\nCode\n# * DOC\nround(123.445, 2)\n\n\n123.44\n\n\n\n\nCode\n# * DOC\nfix_round_and_format(123.445, 2, FormatOptions())\n\n\n'123.45'\n\n\n\n\nCode\n# * DOC\nfix_round_and_format(123.445, 2, FormatOptions(prefix=\"$\"))\n\n\n'$123.45'\n\n\n\n\nCode\n# * DOC\nfix_round_and_format(123.445, 2, FormatOptions(suffix=\"$\"))\n\n\n'123.45$'\n\n\n\n\nCode\n# * DOC\nfix_round_and_format(9123.445, 2, FormatOptions(suffix=\"$\", comma_str=\" \"))\n\n\n'9 123.45$'\n\n\n\n\nCode\n# * DOC\nfix_round_and_format(\n    9123.445, 2, FormatOptions(suffix=\"$\", comma_str=\".\", decimal_str=\",\")\n)\n\n\n'9.123,45$'\n\n\n\nsource\n\n\nround_and_format\n\n round_and_format (num:float, position:int,\n                   format_opts:__main__.FormatOptions)\n\n…\n\n\n\n\nType\nDetails\n\n\n\n\nnum\nfloat\nfloat\n\n\nposition\nint\nint\n\n\nformat_opts\nFormatOptions\nFormatOptions\n\n\nReturns\ntyping.Union[str, float]\nUnion[str, float]\n\n\n\n\n\n\n02 format dates\n\nsource\n\ndate_to_string\n\n date_to_string\n                 (t_val:Union[pandas._libs.tslibs.timestamps.Timestamp,dat\n                 etime.datetime], long:bool=True, period:bool=False)\n\n…\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nt_val\ntyping.Union[pandas._libs.tslibs.timestamps.Timestamp, datetime.datetime]\n\nUnion[pd.Timestamp, datetime]\n\n\nlong\nbool\nTrue\nbool\n\n\nperiod\nbool\nFalse\nbool\n\n\nReturns\nstr\n\nstr\n\n\n\n\n\nCode\n# * TEST\nx = datetime(2024, 3, 19, 16, 39, 2, 126548)\nassert date_to_string(x) == \"March 19, 2024\"\nassert date_to_string(x, long=False) == \"Mar 19, 2024\"\nassert date_to_string(x, period=True, long=False) == \"Mar. 19, 2024\"\n\n\n\nsource\n\n\nconvert_excel_date\n\n convert_excel_date (ordinal:Union[int,float])\n\n…\n\n\n\n\nType\nDetails\n\n\n\n\nordinal\ntyping.Union[int, float]\nfloat - eg. 45292\n\n\nReturns\ndatetime\ndatetime\n\n\n\n\n\nCode\n# * TEST\nassert convert_excel_date(0) == datetime(1899, 12, 31, 0, 0)\nassert convert_excel_date(2) == datetime(1900, 1, 2, 0, 0)\nassert convert_excel_date(45292) == datetime(2024, 1, 1, 0, 0)\nassert convert_excel_date(45292.0) == datetime(2024, 1, 1, 0, 0)\nassert convert_excel_date(45292.25) == datetime(2024, 1, 1, 6, 0)\n\n\n\nsource\n\n\ndate_to_french\n\n date_to_french (date_obj:Union[pandas._libs.tslibs.timestamps.Timestamp,d\n                 atetime.datetime], long:bool=True)\n\n…\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndate_obj\ntyping.Union[pandas._libs.tslibs.timestamps.Timestamp, datetime.datetime]\n\nUnion[pd.Timestamp, datetime]\n\n\nlong\nbool\nTrue\nbool\n\n\nReturns\nstr\n\nstr\n\n\n\n\n\nCode\n# * TEST\nassert date_to_french(datetime(1899, 12, 31, 0, 0)) == \"31 décembre 1899\"\nassert date_to_french(datetime(1899, 12, 31, 0, 0), long=False) == \"31 déc. 1899\"\n\n\n\nsource\n\n\nconvert_yyyymmdd\n\n convert_yyyymmdd (date_str:str)\n\n…\n\n\n\n\nType\nDetails\n\n\n\n\ndate_str\nstr\nstr - eg. 19810123\n\n\nReturns\nTimestamp\npd.Timestamp\n\n\n\n\n\nCode\n# * TEST\nassert convert_yyyymmdd(\"19120124\") == pd.Timestamp(\"1912-01-24 00:00:00\")\n\n\n\n\n\n03 format addresses\n\nsource\n\nadd_mailing_details_col\n\n add_mailing_details_col (df:pandas.core.frame.DataFrame,\n                          address_dict:dict,\n                          col_name:str='str_mailing_details')\n\nmain\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\npd.DataFrame\n\n\naddress_dict\ndict\n\ndict - see docs for example\n\n\ncol_name\nstr\nstr_mailing_details\nstr - name of column that will be added\n\n\nReturns\nDataFrame\n\npd.DataFrame\n\n\n\n\n\nCode\n# * TEST\ndef prep_data0() -&gt; pd.DataFrame:\n    \"\"\"prep data for comparison - add dropped col `Address-3` back\"\"\"\n    df = pd.read_parquet(fp(\"../tests/test_dat/bmark_20240205_1721.parquet\"))\n    df[\"str_greeting\"] = df[\"str_sal\"] + \" \" + df[\"Last Name\"].str.strip()\n    df[\"str_full_name\"] = (\n        df[\"First Name\"].str.strip() + \" \" + df[\"Last Name\"].str.strip()\n    )\n    df[\"str_full_name2\"] = df[\"str_sal\"] + \" \" + df[\"str_full_name\"].str.strip()\n    df[\"Address-3\"] = np.nan\n    return df\n\n\naddress_dict = {\n    \"name\": \"str_full_name2\",\n    \"street1\": \"Address-1\",\n    \"street2\": \"Address-2\",\n    \"street3\": \"Address-3\",\n    \"city\": \"Address-4\",\n    \"pc\": \"Postal Code\",\n    \"prov\": \"Province\",\n    \"country\": \"Current Address.Country\",\n}\n\ncompare_df = pd.read_parquet(fp(\"../tests/test_dat/dat.parquet\"))\ndf = prep_data0()\ndf = add_mailing_details_col(df, address_dict)\nassert df[\"str_mailing_details\"].tolist() == compare_df[\"str_mailing_details\"].tolist()\n\n\n\n\nCode\n# * TEST\ndef prep_data1() -&gt; pd.DataFrame:\n    \"\"\"prep data for comparison - leave dropped col `Address-3` as is\"\"\"\n    df = pd.read_parquet(fp(\"../tests/test_dat/bmark_20240205_1721.parquet\"))\n    df[\"str_greeting\"] = df[\"str_sal\"] + \" \" + df[\"Last Name\"].str.strip()\n    df[\"str_full_name\"] = (\n        df[\"First Name\"].str.strip() + \" \" + df[\"Last Name\"].str.strip()\n    )\n    df[\"str_full_name2\"] = df[\"str_sal\"] + \" \" + df[\"str_full_name\"].str.strip()\n    assert \"Address-3\" not in df.columns.tolist()\n    return df\n\n\ncompare_df = pd.read_parquet(fp(\"../tests/test_dat/dat.parquet\"))\ndf = prep_data1()\ndf = add_mailing_details_col(df, address_dict)\nassert df[\"str_mailing_details\"].tolist() == compare_df[\"str_mailing_details\"].tolist()\n\n\nIf there is no 2nd/3rd street column, put n/a (assuming n/a isn’t another column)\n\n\nCode\n# * DOC\n# Sample address_dict, where values are the column names in the data. Assumes a maximum of three columns for street.\naddress_dict = {\n    \"name\": \"str_full_name2\",\n    \"street1\": \"Address-1\",\n    \"street2\": \"Address-2\",\n    \"street3\": \"n/a\",\n    \"city\": \"Address-4\",\n    \"pc\": \"Postal Code\",\n    \"prov\": \"Province\",\n    \"country\": \"Current Address.Country\",\n}\n\ndf = prep_data1()  # load, clean, etc.\ndf = add_mailing_details_col(df, address_dict)\n\n\n\nsource\n\n\nprovince_abbrev\n\n province_abbrev (province_name:str, errors:str='raise')\n\n…\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprovince_name\nstr\n\nstr\n\n\nerrors\nstr\nraise\nstr - raise / coerce (if no match, return None) / ignore (return input)\n\n\nReturns\nstr\n\nstr - province abbreviation\n\n\n\n\n\nCode\n# * TEST\nwith pytest.raises(Exception, match=\"Invalid province name\"):\n    province_abbrev(\"FL\", errors=\"raise\")\nwith pytest.raises(Exception, match=\"Invalid `error` arg\"):\n    province_abbrev(\"FL\", errors=\"test\")\nassert province_abbrev(\"FL\", errors=\"coerce\") is None\nassert province_abbrev(\"FL\", errors=\"ignore\") == \"FL\"\n\nassert province_abbrev(\"Ontario\") == \"ON\"\nassert province_abbrev(\"newfoundland\") == \"NL\"\nassert province_abbrev(\"NEWFOUNDLAND AND LABRADOR\") == \"NL\"\nassert province_abbrev(\"yukon\") == \"YT\"\nassert province_abbrev(\"YUKON TERRITORY\") == \"YT\"\nassert province_abbrev(\"PEI\") == \"PE\"\nassert province_abbrev(\"novascotia\") == \"NS\"",
    "crumbs": [
      "00_core"
    ]
  },
  {
    "objectID": "merge.html",
    "href": "merge.html",
    "title": "01_merge",
    "section": "",
    "text": "source\n\n\n\n merge (cfg_path:str, id_col:str, group_col:str,\n        multi_line_cols:list=['str_mailing_details'], _tbls:list=None,\n        _test:bool=False, _zip:bool=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncfg_path\nstr\n\npath to config yaml file - example in tests2\n\n\nid_col\nstr\n\nname of unique id column\n\n\ngroup_col\nstr\n\nname of column w/ group identifier\n\n\nmulti_line_cols\nlist\n[‘str_mailing_details’]\nOnly needed if multi-line outputs are bugged. Add all columns w/ newline character.\n\n\n_tbls\nlist\nNone\ncorresponds to table name(s) in template\n\n\n_test\nbool\nFalse\nif True, only processes first 5 for quick checking/iteration\n\n\n_zip\nbool\nFalse\nadd result dir to zip file\n\n\nReturns\nNone",
    "crumbs": [
      "01_merge"
    ]
  },
  {
    "objectID": "merge.html#loadclean",
    "href": "merge.html#loadclean",
    "title": "01_merge",
    "section": "load/clean",
    "text": "load/clean\n\n\nCode\n# * DOC\ndf = pd.read_parquet(fp(\"../tests2/fake_dat.parquet\"))\ndf.sample(3)\n\n\n\n\n\n\n\n\n\n\nid\nfname\nlname\ndob\ndob_verified\nemail\nphone\nstreet\ncity\nprov\n...\n__tbl_rows__type_3\n__tbl_rows__type_4\n__tbl_rows__amt_1\n__tbl_rows__amt_2\n__tbl_rows__amt_3\n__tbl_rows__amt_4\n__tbl_rows__details_1\n__tbl_rows__details_2\n__tbl_rows__details_3\n__tbl_rows__details_4\n\n\n\n\n78\n6834930\nWilliam\nVasquez\n1914-11-20\nTrue\nmperez@example.org\n911 565 0371\n33854 Laura Way\nMichaelhaven\nNewfoundland and Labrador\n...\nNone\nNone\n$4,875.28\n$22,942.55\nNone\nNone\nDirectly deposited to your bank account less a...\nRBC\\n6801 51 ST\\nCOLD LAKE, AB T9M 1Z9\\n\nNone\nNone\n\n\n105\n5135108\nElizabeth\nWilson\n1995-11-11\nTrue\ndavilapamela@example.org\n510.745.2155\n6324 Potter Inlet Suite 671\nCameronhaven\nBritish Columbia\n...\nNone\nNone\n$1,126.23\n$28,237.12\nNone\nNone\nCheque mailed to your address less applicable ...\nCIBC\\n134 MACKENZIE RD.\\nP.O. BOX 1250\\nINUVIK...\nNone\nNone\n\n\n55\n9300534\nCathy\nCastillo\n1996-02-06\nTrue\nbruceblair@example.net\n388-635-2658 x177\n7157 Carrillo Roads Suite 786\nLake Amber\nAlberta\n...\nNone\nNone\n$13,361.36\n$81,729.00\nNone\nNone\nDirectly deposited to your bank account less a...\nRBC ROYAL BANK\\n78 ELIZABETH DR\\nGANDER, NL A...\nNone\nNone\n\n\n\n\n3 rows × 25 columns\n\n\n\n\n\n\nCode\n# * DOC\ndf[\"dt_send\"] = \"May 31, 2024\"\n\ndf[\"str_full_name\"] = df[\"fname\"].str.strip() + \" \" + df[\"lname\"].str.strip()\ndf[\"country\"] = \"Canada\"\ndf[\"prov\"] = df[\"prov\"].apply(lambda x: mm.province_abbrev(x))\n\naddress_dict = {\n    \"name\": \"str_full_name\",\n    \"street1\": \"street\",\n    \"street2\": \"n/a\",\n    \"street3\": \"n/a\",\n    \"city\": \"city\",\n    \"pc\": \"pc\",\n    \"prov\": \"prov\",\n    \"country\": \"country\",\n}\ndf = mm.add_mailing_details_col(df, address_dict, col_name=\"str_mailing_details\")\n\nfor _ in df[\"str_mailing_details\"].tolist()[:3]:\n    print(_, \"\\n\")\n\n\nHENRY ALVARADO\n91971 WILLIAMS SHOALS\nPORT RICHARD, SK  M6J 1P4\nCANADA \n\nROBERT HENRY\n54198 LAMBERT ISLE\nSTEVENSSTAD, NU  M2M 3L2\nCANADA \n\nJENNIFER ROSS\n10535 BARTLETT HILLS\nMCKNIGHTCHESTER, MB  T8K 6J5\nCANADA \n\n\n\n\n\nCode\n# * DOC\ndf[\"dt_dob\"] = df[\"dob\"].apply(lambda x: mm.date_to_string(x))\ndf[\"bool_dob_verified\"] = df[\"dob_verified\"]\n\ndf[\"bool_some_bool\"] = 0\ndf[\"bool_some_bool\"] = df[\"bool_some_bool\"].apply(lambda x: random.randint(0, 1))\ndf[\"bool_some_bool\"] = df[\"bool_some_bool\"].apply(lambda x: True if x == 1 else False)\n\ndf[\"str_comment\"] = df[\"comment\"]\n\nm = df[\"bool_some_bool\"] == True\ndf.loc[~m, \"_group\"] = \"group1\"\ndf.loc[m, \"_group\"] = \"group2\"\n\ndf.columns = [\n    \"id\",\n    \"fname\",\n    \"lname\",\n    \"dob\",\n    \"dob_verified\",\n    \"email\",\n    \"phone\",\n    \"street\",\n    \"city\",\n    \"prov\",\n    \"pc\",\n    \"comment\",\n    \"amt_total\",\n    \"__tbl1__type_1\",\n    \"__tbl1__type_2\",\n    \"__tbl1__type_3\",\n    \"__tbl1__type_4\",\n    \"__tbl1__amt_1\",\n    \"__tbl1__amt_2\",\n    \"__tbl1__amt_3\",\n    \"__tbl1__amt_4\",\n    \"__tbl1__details_1\",\n    \"__tbl1__details_2\",\n    \"__tbl1__details_3\",\n    \"__tbl1__details_4\",\n    \"dt_send\",\n    \"str_full_name\",\n    \"country\",\n    \"str_mailing_details\",\n    \"dt_dob\",\n    \"bool_dob_verified\",\n    \"bool_some_bool\",\n    \"str_comment\",\n    \"_group\",\n]\n\ndf[\"str_comment\"] = df[\"str_comment\"].apply(lambda x: x + \"\\n\\n\")\n\ndf.to_parquet(\"../tests2/_clean_dat.parquet\")",
    "crumbs": [
      "01_merge"
    ]
  },
  {
    "objectID": "merge.html#note-on-tables",
    "href": "merge.html#note-on-tables",
    "title": "01_merge",
    "section": "note on tables",
    "text": "note on tables\nFor tables, the original data would typically look something like this:\n\ndata = {\n    \"id\": [99, 99, 100, 100, 100, 100],\n    \"type\": [\"A\", \"C\", \"A\", \"B\", \"A\", \"B\"],\n    \"amt\": [10, 20, 30, 40, 50, 60],\n}\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\n\nid\ntype\namt\n\n\n\n\n0\n99\nA\n10\n\n\n1\n99\nC\n20\n\n\n2\n100\nA\n30\n\n\n3\n100\nB\n40\n\n\n4\n100\nA\n50\n\n\n5\n100\nB\n60\n\n\n\n\n\n\n\n\nTo get all table rows into one row per id:\n\n\nCode\n# * DOC\ndf[\"num\"] = df.groupby(\"id\").cumcount() + 1\ndf_pivot = df.pivot(index=\"id\", columns=\"num\", values=[\"type\", \"amt\"])\ndf_pivot.columns = [f\"__tbl1__{col[0]}_{col[1]}\" for col in df_pivot.columns]\n# where tbl1 corresponds to name of table in template\ndf_pivot = df_pivot.reset_index().copy()\ndf_pivot  # merge this back into rest of data\n\n\n\n\n\n\n\n\n\n\nid\n__tbl1__type_1\n__tbl1__type_2\n__tbl1__type_3\n__tbl1__type_4\n__tbl1__amt_1\n__tbl1__amt_2\n__tbl1__amt_3\n__tbl1__amt_4\n\n\n\n\n0\n99\nA\nC\nNaN\nNaN\n10\n20\nNaN\nNaN\n\n\n1\n100\nA\nB\nA\nB\n30\n40\n50\n60",
    "crumbs": [
      "01_merge"
    ]
  },
  {
    "objectID": "merge.html#test-2-tables",
    "href": "merge.html#test-2-tables",
    "title": "01_merge",
    "section": "test 2 tables",
    "text": "test 2 tables\n\n\nCode\n# * DOC\ndef _create_random_benes(id: str) -&gt; pd.DataFrame:\n    from faker import Faker\n\n    fake = Faker()\n    df = pd.DataFrame(columns=[\"id\", \"name\", \"pct\"])\n\n    num_benes = random.randint(1, 4)\n\n    for i in range(num_benes):\n        df.loc[len(df)] = [id, fake.name(), f\"{round(100/num_benes,2)}%\"]\n\n    return df\n\n\nif REFRESH:\n    df = pd.read_parquet(\"../tests2/clean_dat.parquet\")\n    dfs = []\n    for _id in df[\"id\"].tolist():\n        dfs.append(_create_random_benes(_id))\n    benes = pd.concat(dfs)\n\n    benes[\"num\"] = benes.groupby(\"id\").cumcount() + 1\n    benes_pivot = benes.pivot(index=\"id\", columns=\"num\", values=[\"name\", \"pct\"])\n    benes_pivot.columns = [f\"__tbl2__{col[0]}_{col[1]}\" for col in benes_pivot.columns]\n    benes_pivot = benes_pivot.reset_index().copy()\n\n    tm = pd.merge(df, benes_pivot, \"inner\", \"id\")\n    assert len(tm) == len(df)\n    assert len(tm) == len(benes_pivot)\n\n    tm.to_parquet(\"../tests2/clean_dat2.parquet\")",
    "crumbs": [
      "01_merge"
    ]
  },
  {
    "objectID": "merge.html#run-merge",
    "href": "merge.html#run-merge",
    "title": "01_merge",
    "section": "run merge",
    "text": "run merge\nConfig is in yaml format:\n\n\n\nscreenshot.png\n\n\nThe config file is used to specify the data source (currently, only parquet is supported to ensure consistent/persistent data types), groups, and what template(s) each group should use.\n\n\nCode\n# * DOC\ncfg_path = \"../tests2/sample_cfg.yml\"\ncfg = yaml_helper(cfg_path)\npprint(cfg)\n\n\n{'data': 'C:\\\\Users\\\\ronal\\\\Desktop\\\\mail_merge\\\\tests2\\\\clean_dat2.parquet',\n 'groups': [{'group1': ['C:\\\\Users\\\\ronal\\\\Desktop\\\\mail_merge\\\\tests2\\\\tpls\\\\sample_tpl.docx',\n                        'C:\\\\Users\\\\ronal\\\\Desktop\\\\mail_merge\\\\tests2\\\\tpls\\\\sample_tpl2.docx']},\n            {'group2': ['C:\\\\Users\\\\ronal\\\\Desktop\\\\mail_merge\\\\tests2\\\\tpls\\\\sample_tpl.docx']}]}\n\n\n\n\nCode\n# * DOC\nif REFRESH:\n    merge(\n        cfg_path=\"../tests2/sample_cfg.yml\",\n        id_col=\"id\",\n        group_col=\"_group\",\n        _tbls=[\"tbl1\", \"tbl2\"],\n        _test=True,\n        _zip=True,\n    )\n\n\n\n\n\n\n\n\n\n\n\nSample result in /tests2/res_2024...zip\nSample templates used in /tests2/tpls/",
    "crumbs": [
      "01_merge"
    ]
  }
]